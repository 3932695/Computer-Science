<!DOCTYPE HTML PUBLIC "-//W3O//DTD W3 HTML 2.0//EN">
<!Converted with LaTeX2HTML 95.1 (Fri Jan 20 1995) by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds >
<HEAD>
<TITLE>2.3 Communication</TITLE>
</HEAD>
<BODY>
<meta name="description" value="2.3 Communication">
<meta name="keywords" value="book">
<meta name="resource-type" value="document">
<meta name="distribution" value="global">
<P>
 <BR> <HR><a href="../../../tppmsgs/msgs0.htm#1" tppabs="http://www.mcs.anl.gov/dbpp/"><img ALIGN=MIDDLE src="pictures//asm_color_tiny.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//asm_color_tiny.gif" alt="[DBPP]"></a>    <A NAME=tex2html2047 HREF="node16.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node16.html"><IMG ALIGN=MIDDLE ALT="previous" SRC="pictures//previous_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//previous_motif.gif"></A> <A NAME=tex2html2055 HREF="node18.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node18.html"><IMG ALIGN=MIDDLE ALT="next" SRC="pictures//next_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//next_motif.gif"></A> <A NAME=tex2html2053 HREF="node14.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node14.html"><IMG ALIGN=MIDDLE ALT="up" SRC="pictures//up_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//up_motif.gif"></A> <A NAME=tex2html2057 HREF="node1.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node1.html"><IMG ALIGN=MIDDLE ALT="contents" SRC="pictures//contents_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//contents_motif.gif"></A> <A NAME=tex2html2058 HREF="node133.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node133.html"><IMG ALIGN=MIDDLE ALT="index" SRC="pictures//index_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//index_motif.gif"></A> <a href="../../../tppmsgs/msgs0.htm#2" tppabs="http://www.mcs.anl.gov/dbpp/search.html"><img ALIGN=MIDDLE src="pictures//search_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//search_motif.gif" alt="[Search]"></a>   <BR>
<B> Next:</B> <A NAME=tex2html2056 HREF="node18.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node18.html">2.4 Agglomeration</A>
<B>Up:</B> <A NAME=tex2html2054 HREF="node14.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node14.html">2 Designing Parallel Algorithms</A>
<B> Previous:</B> <A NAME=tex2html2048 HREF="node16.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node16.html">2.2 Partitioning</A>
<BR><HR><P>
<H1><A NAME=SECTION02330000000000000000>2.3 Communication</A></H1>
<P>
<A NAME=seccomm>&#160;</A>
<P>
<A NAME=1158>&#160;</A>
The tasks generated by a partition are intended to execute
concurrently but cannot, in general, execute independently.  The
computation to be performed in one task will typically require data
associated with another task.  Data must then be transferred between
tasks so as to allow computation to proceed.  This information flow is
specified in the <em> communication
 </em> phase of a design.
<P>
Recall from Chapter <A HREF="node6.html#chap1" tppabs="http://www.mcs.anl.gov/dbpp/text/node6.html#chap1">1</A> that in our programming model, we
conceptualize a need for communication between two tasks as a 
<A NAME=1161>&#160;</A>
channel linking the tasks, on which one task can send messages
<A NAME=1162>&#160;</A>
and from which the other can receive.  Hence, the communication
associated with an algorithm can be specified in two phases.  First,
we define a channel structure that links, either directly or
indirectly, tasks that require data (consumers) with tasks that
possess those data (producers).  Second, we specify the messages that
are to be sent and received on these channels.  Depending on our
eventual implementation technology, we may not actually create these
channels when coding the algorithm. For example, in a data-parallel
language, we simply specify data-parallel operations and data
distributions.  Nevertheless, thinking in terms of tasks and channels
helps us to think quantitatively about locality issues and
communication costs.
<P>
The definition of a channel involves an intellectual cost and the
sending of a message involves a physical cost.  Hence, we avoid
introducing unnecessary channels and communication operations.  In
addition, we seek to optimize performance by distributing
communication operations over many tasks and by organizing
communication operations in a way that permits concurrent execution.
<P>
<A NAME=1163>&#160;</A>
In domain decomposition problems, communication requirements can be
difficult to determine.  Recall that this strategy produces tasks by
first partitioning data structures into disjoint subsets and then
associating with each datum those operations that operate solely on
that datum.  This part of the design is usually simple.  However, some
operations that require data from several tasks usually remain.
Communication is then required to manage the data transfer necessary
for these tasks to proceed.  Organizing this communication in an
efficient manner can be challenging.  Even simple decompositions can
have complex communication structures.
<P>
<A NAME=1164>&#160;</A>
In contrast, communication requirements in parallel algorithms
obtained by functional decomposition are often straightforward: they
correspond to the data flow between tasks.  For example, in a climate
model broken down by functional decomposition into atmosphere model,
ocean model, and so on, the communication requirements will correspond to
the interfaces between the component submodels: the atmosphere model
will produce values that are used by the ocean model, and so on
(Figure <A HREF="node16.html#figenv" tppabs="http://www.mcs.anl.gov/dbpp/text/node16.html#figenv">2.3</A>).
<P>
In the following discussion, we use a variety of examples to show how
communication requirements are identified and how channel structures
and communication operations are introduced to satisfy these
requirements.  For clarity in exposition, we categorize communication
<A NAME=1166>&#160;</A>
patterns along four loosely orthogonal axes: local/global,
structured/unstructured, static/dynamic, and synchronous/asynchronous.
<UL><LI>
<A NAME=1168>&#160;</A>
In <em> local
 </em> communication, each task communicates with a small
set of other tasks (its ``neighbors''); in contrast, <em> global
 </em>
communication requires each task to communicate with many tasks.
<LI>
<A NAME=1171>&#160;</A>
In <em> structured
 </em> communication, a task and its neighbors form
a regular structure, such as a tree or grid; in contrast, <em>
unstructured
 </em> communication networks may be arbitrary graphs.
<LI>
<A NAME=1174>&#160;</A>
In <em> static
 </em> communication, the identity of communication partners
does not change over time; in contrast, the identity of communication
<A NAME=1176>&#160;</A>
partners in <em> dynamic
 </em> communication structures may be determined
by data computed at runtime and may be highly variable.
<LI>
<A NAME=1178>&#160;</A>
In <em> synchronous
 </em> communication, producers and consumers execute in
a coordinated fashion, with producer/consumer pairs cooperating in
<A NAME=1180>&#160;</A>
data transfer operations; in contrast, <em> asynchronous
 </em>
communication may require that a consumer obtain data without the
cooperation of the producer.
</UL>
<A NAME=1183>&#160;</A>
<P>

<P>
<H2><A NAME=SECTION02331000000000000000>2.3.1 Local Communication</A></H2>
<P>
<A NAME=seclocal>&#160;</A>
<P>
<A NAME=1186>&#160;</A>
A local communication structure is obtained when an operation requires
data from a small number of other tasks.  It is then straightforward
to define channels that link the task responsible for performing the
operation (the consumer) with the tasks holding the required data (the
producers) and to introduce appropriate send and receive operations
in the producer and consumer tasks, respectively.
<P>
<A NAME=1187>&#160;</A>
For illustrative purposes, we consider the communication requirements
associated with a simple numerical computation, namely a Jacobi finite
difference method.  In this class of numerical method, a
multidimensional grid is repeatedly updated by replacing the value at
each point with some function of the values at a small, fixed number
of neighboring points.  The set of values required to update a single
<A NAME=1188>&#160;</A>
grid point is called that grid point's <em> stencil</em>.  For example,
the following expression uses a five-point stencil to update each
element <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img163.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img163.gif"> of a two-dimensional grid <em> X</em>
:
<P><A NAME=eq5pt>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img164.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img164.gif"><P>
This update is applied repeatedly to compute a sequence of values
<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img165.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img165.gif">, <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img166.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img166.gif">, and so on.  The notation <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img167.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img167.gif">
denotes the value of grid point <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img168.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img168.gif"> at step <em> t</em>
.
<P>
<P><A NAME=2406>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img169.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img169.gif">
<BR><STRONG>Figure 2.4:</STRONG> <em> Task and channel structure for a two-dimensional finite
difference computation with five-point update stencil.  In this simple
fine-grained formulation, each task encapsulates a single element of a
two-dimensional grid and must both send its value to four neighbors
and receive values from four neighbors.  Only the channels used by the
shaded task are shown.</em><A NAME=figsten4>&#160;</A><BR>
<P>
<P>
Let us assume that a partition has used domain decomposition
techniques to create a distinct task for each point in the
two-dimensional grid.  Hence, a task allocated the grid point
<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img170.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img170.gif"> must compute the sequence
<PRE><TT> 
		 <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img171.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img171.gif"> 
</TT></PRE>
This computation requires in turn the four corresponding sequences
<PRE><TT> 
		 <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img172.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img172.gif"> 
</TT></PRE>
<PRE><TT> 
		 <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img173.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img173.gif"> 
</TT></PRE>
<PRE><TT> 
		 <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img174.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img174.gif"> 
</TT></PRE>
<PRE><TT> 
		 <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img175.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img175.gif"> 
</TT></PRE>
which are produced by the four tasks handling grid points <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img176.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img176.gif">,
<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img177.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img177.gif">, <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img178.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img178.gif">, and <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img179.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img179.gif">, that is, by its four
neighbors in the grid.  For these values to be communicated, we define
channels linking each task that requires a value with the task that
generates that value.  This yields the channel structure illustrated
in Figure <A HREF="node17.html#figsten4" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#figsten4">2.4</A>.  Each task then executes the following
logic:
<P>

<P>

<PRE><TT> 
		 <tt> for</tt> <em> t=0</em>
 <tt> to</tt> <em> T-1</em>
<P>
				 <tt> send</tt> <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img180.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img180.gif"> <tt> to each neighbor</tt>
<P>
				 <tt> receive</tt> <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img181.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img181.gif">, <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img182.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img182.gif">,
	     <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img183.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img183.gif">, <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img184.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img184.gif"> <tt> from neighbors</tt>
<P>
				 <tt> compute</tt> <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img185.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img185.gif"> <tt> using Equation <A HREF="node17.html#eq5pt" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#eq5pt">2.1</A></tt>
<P>
		 <tt> endfor</tt>
<P>
</TT></PRE>

<P>
We observed earlier that the best sequential and parallel solutions to
a problem may use different techniques.  This situation arises in
finite difference problems.  In sequential computing, 
<A NAME=1293>&#160;</A>
Gauss-Seidel update strategies are often preferred over Jacobi
strategies because they allow solutions of comparable accuracy to be
obtained using fewer iterations.  In a Gauss-Seidel scheme, elements
are updated in a particular order so that the computation of each
element can use the most up-to-date value of other elements.  For
<A NAME=1294>&#160;</A>
example, the Jacobi update of Equation <A HREF="node17.html#eq5pt" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#eq5pt">2.1</A> may be reformulated
as follows (notice the use of values <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img186.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img186.gif"> and
<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img187.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img187.gif">):
<P><A NAME=eq5pta>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img188.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img188.gif"><P>
<P>
While Jacobi schemes are trivial to parallelize (all grid points can
be updated concurrently), this is not the case for all Gauss-Seidel
<A NAME=1316>&#160;</A>
schemes.  For example, the update scheme of Equation <A HREF="node17.html#eq5pta" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#eq5pta">2.2</A> allows
only an average of around <em> N/2</em>
 points within an
<em> N</em>
<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img189.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img189.gif"><em> N</em>
 grid to be updated concurrently.  Fortunately,
many different Gauss-Seidel orderings are possible for most problems,
and we are usually free to choose the ordering that maximizes
available parallelism.  In particular, we can choose to update first
the odd-numbered elements and then the even-numbered
elements of an array.  Each update uses the most recent information,
yet the updates to the odd-numbered points are independent and can
proceed concurrently, as can the updates to the even-numbered points.
<A NAME=1321>&#160;</A> 
This update strategy yields what is referred to as a <em> red-black</em>
algorithm, since the points can be thought of as being colored as on a
chess board: either red (odd) or black (even); points of the same
color can be updated concurrently.  Figure <A HREF="node17.html#figjrb" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#figjrb">2.5</A> illustrates
both the Gauss-Seidel scheme of Equation <A HREF="node17.html#eq5pta" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#eq5pta">2.2</A> and a red-black
scheme, and shows how the latter scheme increases opportunities for
parallel execution.
<P>
<P><A NAME=2426>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img190.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img190.gif">
<BR><STRONG>Figure 2.5:</STRONG> <em> Two finite difference update strategies, here applied on a
two-dimensional grid with a five-point stencil.  In both figures,
shaded grid points have already been updated to step <em> t+1</em>
;
unshaded grid points are still at step <em> t</em>
.  The arrows show data
dependencies for one of the latter points.  The figure on the left
illustrates a simple Gauss-Seidel scheme and highlights the five grid
points that can be updated at a particular point in time.  In this
scheme, the update proceeds in a wavefront from the top left corner to
the bottom right.  On the right, we show a red-black update scheme.
Here, all the grid points at step <em> t</em>
 can be updated
concurrently.</em><A NAME=figjrb>&#160;</A><BR>
<P>
<P>
This example indicates the important role that choice of solution
strategy can play in determining the performance of a parallel
program.  While the simple Gauss-Seidel update strategy of
Equation <A HREF="node17.html#eq5pta" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#eq5pta">2.2</A> may be appropriate in a sequential program, it
is not ideal on a parallel computer.  The Jacobi update strategy is
efficient on a parallel computer but is inferior numerically.  The
red-black scheme combines the advantages of both approaches.
<A NAME=1331>&#160;</A>
<P>

<P>
<H2><A NAME=SECTION02332000000000000000>2.3.2 Global Communication</A></H2>
<P>
<A NAME=seccommglobal>&#160;</A>
<P>
<P><A NAME=2447>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img191.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img191.gif">
<BR><STRONG>Figure 2.6:</STRONG> <em> A centralized summation algorithm that uses a central
manager task (S) to sum <em> N</em>
 numbers distributed among
<em> N</em>
 tasks.  Here, <em> N=8</em>
, and each of the 8 channels is labeled with the
number of the step in which they are used.</em><A NAME=figsum>&#160;</A><BR>
<P>
<P>
<A NAME=1340>&#160;</A>
A <i> global communication</i> operation is one in which many tasks must
participate.  When such operations are implemented, it may not be
sufficient simply to identify individual producer/consumer pairs.
Such an approach may result in too many communications or may restrict
opportunities for concurrent execution.  For example, consider the
<A NAME=1342>&#160;</A>
problem of performing a <em> parallel reduction
 </em> operation, that
<A NAME=1344>&#160;</A>
is, an operation that reduces <em> N</em>
 values distributed over
<em> N</em>
 tasks using a commutative associative operator such as addition:
<P><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img192.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img192.gif"><P>
Let us assume that a single ``manager'' task requires the result
<em> S</em>
 of this operation.  Taking a purely local view of
communication, we recognize that the manager requires values <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img193.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img193.gif">,
<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img194.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img194.gif">, etc., from tasks 0, 1, etc.  Hence, we could define a
communication structure that allows each task to communicate its value
to the manager independently.  The manager would then receive the
values and add them into an accumulator (Figure <A HREF="node17.html#figsum" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#figsum">2.6</A>).
However, because the manager can receive and sum only one number at a
time, this approach takes <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img195.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img195.gif"> time to sum <em> N</em>
 numbers---not a
very good parallel algorithm!
<P>
<A NAME=1354>&#160;</A>
This example illustrates two general problems that can hinder
efficient parallel execution in algorithms based on a purely local
view of communication:
<OL><LI>
The algorithm is <em> centralized
 </em>: it does not distribute
computation and communication.  A single task (in this case, the
manager task) must participate in every operation.
<LI>
The algorithm is <em> sequential
 </em>: it does not allow multiple
computation and communication operations to proceed concurrently.
</OL>
We must address both these problems to develop a good parallel
algorithm.
<P>
<H4><A NAME=SECTION02332010000000000000> Distributing Communication and Computation.</A></H4>
<P>
We first consider the problem of distributing the computation and
communication associated with the summation.  We can distribute the
summation of the <em> N</em>
 numbers by making each task <em> i</em>
,
<em> 0&lt;i&lt;N-1</em>
, compute the sum:
<P><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img196.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img196.gif"><P>
<P>
<P><A NAME=2475>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img197.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img197.gif">
<BR><STRONG>Figure 2.7:</STRONG> <em> A summation algorithm that connects <em> N</em>
 tasks in an
array in order to sum <em> N</em>
 numbers distributed among these tasks.
Each channel is labeled with the number of the step in which it is
used and the value that is communicated on it.</em><A NAME=figsumA>&#160;</A><BR>
<P>
<P>
The communication requirements associated with this algorithm can be
satisfied by connecting the <em> N</em>
 tasks in a one-dimensional array
(Figure <A HREF="node17.html#figsumA" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#figsumA">2.7</A>).  Task <em> N-1</em>
 sends its value to its neighbor
in this array.  Tasks 1 through <em> N-2</em>
 each wait to receive a
partial sum from their right-hand neighbor, add this to their local
value, and send the result to their left-hand neighbor.  Task 0
receives a partial sum and adds this to its local value to obtain the
complete sum.  This algorithm distributes the <em> N-1</em>
 communications
and additions, but permits concurrent execution only if multiple
summation operations are to be performed.  (The array of tasks can
then be used as a pipeline, through which flow partial sums.)  A
single summation still takes <em> N-1</em>
 steps.
<P>
<H4><A NAME=SECTION02332020000000000000> Uncovering Concurrency: Divide and Conquer.</A></H4>
<P>
Opportunities for concurrent computation and communication can often
be uncovered by applying a problem-solving strategy called <em> divide
and conquer</em>.  To solve a complex problem (such as summing
<em> N</em>
 numbers), we seek to partition it into two or more simpler problems of
roughly equivalent size (e.g., summing <em> N/2</em>
 numbers).  This
process is applied recursively to produce a set of subproblems that
cannot be subdivided further (e.g., summing two numbers).  The
strategy is summarized in Algorithm <A HREF="node17.html#algdac" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#algdac">2.1</A>.  The
divide-and-conquer technique is effective in parallel computing when
the subproblems generated by problem partitioning can be solved
concurrently.  For example, in the summation problem, we can take
<A NAME=1382>&#160;</A>
advantage of the following identity (<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img198.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img198.gif">, <em> n</em>
 an
integer):
<P><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img199.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img199.gif"><P>
The two summations on the right hand side can be performed
concurrently.  They can also be further decomposed if <em> n&gt;1</em>
, to
give the tree structure illustrated in Figure <A HREF="node17.html#figsum1" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#figsum1">2.8</A>.
Summations at the same level in this tree of height <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img200.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img200.gif"> can be
performed concurrently, so the complete summation can be achieved in
<IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img201.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img201.gif"> rather than <em> N</em>
 steps.
<P>
<P><A NAME=algdac>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img202.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img202.gif"><P>
<P>
<P><A NAME=2525>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img205.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img205.gif">
<BR><STRONG>Figure 2.8:</STRONG> <em> Tree structure for divide-and-conquer summation algorithm
with <em> N=8</em>
.  The <em> N</em>
 numbers located in the tasks at the
bottom of the diagram are communicated to the tasks in the row
immediately above; these each perform an addition and then forward the
result to the next level.  The complete sum is available at the root
of the tree after <IMG BORDER=0 ALIGN=MIDDLE ALT="" SRC="img204.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img204.gif"> steps.</em><A NAME=figsum1>&#160;</A><BR>
<P>
<P>
In summary, we observe that in developing an efficient parallel
summation algorithm, we have distributed the
<em> N-1</em>
 communication and computation operations required to perform the
summation and have modified the order in which these operations are
performed so that they can proceed concurrently.  The result is a
regular communication structure in which each task communicates with a
small set of neighbors.
<P>

<H2><A NAME=SECTION02333000000000000000>2.3.3 Unstructured and Dynamic Communication</A></H2>
<P>
<A NAME=seccommunstruc>&#160;</A>
<P>
<A NAME=1413>&#160;</A>
<P>
<P><A NAME=2543>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img206.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img206.gif">
<BR><STRONG>Figure 2.9:</STRONG> <em> Example of a problem requiring unstructured communication.
In this finite element mesh generated for an assembly part, each
vertex is a grid point.  An edge connecting two vertices represents a
data dependency that will require communication if the vertices are
located in different tasks.  Notice that different vertices have
varying numbers of neighbors.  (Image courtesy of M.
S. Shephard.)</em><A NAME=figfem>&#160;</A><BR>
<P>
<P>
The examples considered previously are all of static,
structured communication, in which a task's communication partners
form a regular pattern such as a tree or a grid and do not change over
time.  In other cases, communication patterns may be considerably more
<A NAME=1418>&#160;</A>
complex.  For example, in finite element methods used in engineering
<A NAME=1419>&#160;</A>
calculations, the computational grid may be shaped to follow an
irregular object or to provide high resolution in critical regions
(Figure <A HREF="node17.html#figfem" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#figfem">2.9</A>).  Here, the channel structure representing the
communication partners of each grid point is quite irregular and
data-dependent and, furthermore, may change over time if the grid is
refined as a simulation evolves.
<P>
<A NAME=1421>&#160;</A>
Unstructured communication patterns do not generally cause conceptual
difficulties in the early stages of a design.  For example, it is
straightforward to define a single task for each vertex in a finite
element graph and to require communication for each edge.  However,
unstructured communication complicates the tasks of agglomeration and
mapping.  In particular, sophisticated algorithms can be required to
determine an agglomeration strategy that both creates tasks of
approximately equal size and minimizes communication requirements by
creating the least number of intertask edges.  Algorithms of this sort
are discussed in Section <A HREF="node19.html#seclbalgs" tppabs="http://www.mcs.anl.gov/dbpp/text/node19.html#seclbalgs">2.5.1</A>.  If communication
<A NAME=1423>&#160;</A>
requirements are dynamic, these algorithms must be applied frequently
during program execution, and the cost of these algorithms must be
weighed against their benefits.
<P>

<H2><A NAME=SECTION02334000000000000000>2.3.4 Asynchronous Communication</A></H2>
<P>
<A NAME=seccommas>&#160;</A>
<P>
The examples considered in the preceding section have all featured
synchronous communication, in which both producers and consumers are
aware when communication operations are required, and producers
explicitly send
<A NAME=1426>&#160;</A>
data to consumers.  In <em> asynchronous
 </em> communication, tasks that
<A NAME=1428>&#160;</A>
possess data (producers) are not able to determine when other tasks
(consumers) may require data; hence, consumers must explicitly request
data from producers.
<P>
<P><A NAME=2558>&#160;</A><IMG BORDER=0 ALIGN=BOTTOM ALT="" SRC="img207.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/img207.gif">
<BR><STRONG>Figure 2.10:</STRONG> <em> Using separate ``data tasks'' to service read and write
requests on a distributed data structure.  In this figure, four
computation tasks (C) generate read and write requests to eight data
items distributed among four data tasks (D).  Solid lines represent
requests; dashed lines represent replies.  One compute task and one
data task could be placed on each of four processors so as to
distribute computation and data equitably.</em><A NAME=figasyncdata>&#160;</A><BR>
<P>
<P>
This situation commonly occurs when a computation is structured as a
set of tasks that must periodically read and/or write elements of a
shared data structure.  Let us assume that this data structure is too
large or too frequently accessed to be encapsulated in a single task.
Hence, a mechanism is needed that allows this data structure to be
distributed while supporting asynchronous read and write operations on
its components.  Possible mechanisms include the following.
<P>
<OL><LI>
<A NAME=1434>&#160;</A>
The data structure is distributed among the computational tasks.  Each
<A NAME=1435>&#160;</A>
task both performs computation and generates requests for data located
in other tasks.  It also periodically interrupts its own computation
and <em> polls
 </em> for pending requests.
<P>
<LI>
The distributed data structure is encapsulated in a second set of
tasks responsible only for responding to read and write requests
(Figure <A HREF="node17.html#figasyncdata" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#figasyncdata">2.10</A>).
<P>
<LI>
On a computer that supports a shared-memory programming model,
computational tasks can access shared data without any special
arrangements.  However, care must be taken to ensure that read and
write operations on this shared data occur in the proper order.
<P>
</OL>
<P>
<A NAME=1439>&#160;</A>
Each strategy has advantages and disadvantages; in addition, the
performance characteristics of each approach vary from machine to
machine.  The first strategy can result in convoluted, nonmodular
programs because of the need to intersperse polling operations 
<A NAME=1440>&#160;</A>
throughout application code.  In addition, polling can be an expensive
operation on some computers, in which case we must trade off the cost
of frequent polling against the benefit of rapid response to remote
requests.  The second strategy is more modular: responsibility for the
shared data structure is encapsulated in a separate set of tasks.
However, this strategy makes it hard to exploit locality because,
strictly speaking, there are no local data: all read and write
operations require communication.  Also, switching between the
computation and data tasks can be expensive on some machines.
<P>

<H2><A NAME=SECTION02335000000000000000>2.3.5 Communication Design Checklist</A></H2>
<P>
<A NAME=secrules2>&#160;</A>
<P>
<A NAME=1443>&#160;</A>
Having devised a partition and a communication structure for our
parallel algorithm, we now evaluate our design using the following
design checklist.  As in Section <A HREF="node16.html#secrules1" tppabs="http://www.mcs.anl.gov/dbpp/text/node16.html#secrules1">2.2.3</A>, these are
guidelines intended to identify nonscalable features, rather than hard
and fast rules.  However, we should be aware of when a design violates
them and why.
<A NAME=1445>&#160;</A>
<P>
<OL><LI>
Do all tasks perform about the same number of communication
operations?  Unbalanced communication requirements suggest a
nonscalable construct.  Revisit your design to see whether
communication operations can be distributed more equitably.  For
example, if a frequently accessed data structure is encapsulated in a
single task, consider distributing or replicating this data structure.
<P>
<LI>
Does each task communicate only with a small number of neighbors?  If
each task must communicate with many other tasks, evaluate the
possibility of formulating this global communication in terms of a
local communication structure, as was done in the pairwise
interactions algorithm of Section <A HREF="node10.html#exinteractions" tppabs="http://www.mcs.anl.gov/dbpp/text/node10.html#exinteractions">1.4.2</A> and the
summation algorithm of Section <A HREF="node17.html#seccommglobal" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#seccommglobal">2.3.2</A>.
<P>
<LI>
Are communication operations able to proceed concurrently?  If not,
your algorithm is likely to be inefficient and nonscalable.  Try to
use divide-and-conquer techniques to uncover concurrency, as in the
summation algorithm of Section <A HREF="node17.html#seccommglobal" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#seccommglobal">2.3.2</A>.
<P>
<LI>
Is the computation associated with different tasks able to proceed
concurrently?  If not, your algorithm is likely to be inefficient and
nonscalable.  Consider whether you can reorder communication and
computation operations.  You may also wish to revisit your problem
specification, as was done in moving from a simple Gauss-Seidel to a
red-black algorithm in Section <A HREF="node17.html#seclocal" tppabs="http://www.mcs.anl.gov/dbpp/text/node17.html#seclocal">2.3.1</A>.
</OL>
<P>

<BR> <HR><a href="../../../tppmsgs/msgs0.htm#1" tppabs="http://www.mcs.anl.gov/dbpp/"><img ALIGN=MIDDLE src="pictures//asm_color_tiny.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//asm_color_tiny.gif" alt="[DBPP]"></a>    <A NAME=tex2html2047 HREF="node16.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node16.html"><IMG ALIGN=MIDDLE ALT="previous" SRC="pictures//previous_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//previous_motif.gif"></A> <A NAME=tex2html2055 HREF="node18.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node18.html"><IMG ALIGN=MIDDLE ALT="next" SRC="pictures//next_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//next_motif.gif"></A> <A NAME=tex2html2053 HREF="node14.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node14.html"><IMG ALIGN=MIDDLE ALT="up" SRC="pictures//up_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//up_motif.gif"></A> <A NAME=tex2html2057 HREF="node1.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node1.html"><IMG ALIGN=MIDDLE ALT="contents" SRC="pictures//contents_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//contents_motif.gif"></A> <A NAME=tex2html2058 HREF="node133.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node133.html"><IMG ALIGN=MIDDLE ALT="index" SRC="pictures//index_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//index_motif.gif"></A> <a href="../../../tppmsgs/msgs0.htm#2" tppabs="http://www.mcs.anl.gov/dbpp/search.html"><img ALIGN=MIDDLE src="pictures//search_motif.gif" tppabs="http://www.mcs.anl.gov/dbpp/text/pictures//search_motif.gif" alt="[Search]"></a>   <BR>
<B> Next:</B> <A NAME=tex2html2056 HREF="node18.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node18.html">2.4 Agglomeration</A>
<B>Up:</B> <A NAME=tex2html2054 HREF="node14.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node14.html">2 Designing Parallel Algorithms</A>
<B> Previous:</B> <A NAME=tex2html2048 HREF="node16.html" tppabs="http://www.mcs.anl.gov/dbpp/text/node16.html">2.2 Partitioning</A>
<BR><HR><P>
<P><ADDRESS>
<I>&#169 Copyright 1995 by <A href="../../../tppmsgs/msgs0.htm#3" tppabs="http://www.mcs.anl.gov/people/foster/">Ian Foster</a></I>
</ADDRESS>
</BODY>
